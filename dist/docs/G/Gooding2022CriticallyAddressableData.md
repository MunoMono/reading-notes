---
title: "Towards critically addressable data for digital library user studies"
authors: "Gooding, Paul"
year: 2022
journal: "Archives, Access and Artificial Intelligence: Working with Born-Digital and Digitized Archival Collections"
citation_key: Gooding2022CriticallyAddressableData
doi: "10.1080/0361526"
url: "https://papers.ssrn.com/s"
bibliography: ../../refs/library.bib
csl: "https://www.zotero.org/styles/harvard-cite-them-right"
link-citations: true
last_updated: "Sep 04, 2025, 10:21 AM"---
# Scope of this note
Engages with Chapter 4 of the edited volume, which develops the argument for *critical addressability* in relation to digital library catalogue data. The chapter draws on Gooding’s work in the **Digital Library Futures (DLF)** project and sets out a conceptual framework that resonates directly with the project’s archival and methodological concerns. For the DDR research spine, this text functions at the intersection of **archival analysis, computational modelling and reflective synthesis**.

## Purpose and aim
### What research question or objective is being addressed?
Gooding’s inquiry is explicitly twofold:  
1. What explanatory power does the metaphor of the **black box** bring to understanding library catalogue data in digital library user studies?  
2. How might **data-driven approaches** and critical humanistic perspectives make these black boxes transparent, or 'critically addressable'?】  

The overarching aim is to shift digital library user studies from treating catalogue data as unproblematic input/output flows toward recognising them as **constructed, biased infrastructures** requiring reflexive scrutiny.

## Methodology
### Describe the research design, methods and sample size.
- Empirical base: **Digital Library Futures (DLF)** project, funded by the UK Arts and Humanities Research Council (2017–2019), focusing on Non-Print Legal Deposit (NPLD).  
- Methods:  
  - Expert interviews and practitioner surveys.  
  - **Webometric approaches**: Google Analytics, server logs and subject-based analysis of access requests.  
  - Application of **Dewey Decimal Classification** to analyse access logs from NPLD resources.  
- Conceptual framing: triangulates **black box theory (cybernetics, Latour)**, **Explainable AI (XAI)** debates and the **collections as data** imperative.  
- Epistemology: strongly humanistic, aligned with interpretivist–critical perspectives.  

## Key findings and arguments
### Summarise the main results and conclusions
1. Library usage data is **structurally opaque**: much is generated, transformed and stored without researchers’ visibility of its construction.  
2. **Black box theory** provides a powerful metaphor: catalogues and analytics platforms resemble systems where only input/output are visible, not processes.  
3. **Critical addressability** is proposed as a criterion for evaluating datasets: researchers must know *why* certain data is included/excluded, *who* shaped it and *what tools* enacted transformations【20†source】.  
4. Three 'layers' of addressability are identified:  
   - **Non-addressable datasets** (proprietary tools like Google Analytics).  
   - **Received datasets** (using existing taxonomies such as Dewey Decimal).  
   - **Captured datasets** (where researchers define and document categories).  
5. The **collections as data** framework and FAIR data principles (Findable, Accessible, Interoperable, Reusable) provide complementary humanistic and technical standards.  
6. Interdisciplinary collaboration between LIS, DH and data science is needed to design **humanistic catalogue datasets** that are transparent, reflexive and computationally usable.  

## Relevance
### How does it link to the research questions or framework?
- Directly addresses the **problem of incomplete or biased data** central to DDR systematic design reinterpretation【10†source】.  
- Provides a **diagnostic tool**—critical addressability—that maps neatly onto DDR archival categories and their classificatory biases.  
- Supports the **interpretivist–critical framework**: data is situated, partial and bound by power relations.  
- Resonates with the **taxonomy of design methods** work: Gooding’s concern with classification biases echoes the DDR’s struggle with Archer’s prescriptive structures.  

## Project integration
### Why it helps the project (evidence-linked)
- Offers explicit **criteria for transparency** in datasets, directly applicable to DDR archival analysis and ML/RAG experiments.  
- Reinforces Archer’s own recognition of incomplete/unstable data as structural problems (e.g., hospital bed project).  
- Extends Jaillant’s (2022) argument on archival gaps into a systematic methodology for addressing them.  

### Hooks into the project
- **Workstreams →** Archival interrogation (bias and silences)computational modelling (embedding explainability) → comparative analysis (received vs. captured datasets).  
- **Deliverables →** A practical rubric for assessing DDR archives and contemporary project data by addressability → contributes to the taxonomy of methods.  
- **Stakeholders →** Librarians, archivists, digital humanists, AI researchers and policy-makers concerned with data transparency.  

### Use across the methods spine
- [x] Framing and theory  
- [ ] Study design  
- [x] Data collection and instruments  
- [x] Analysis and models  
- [x] Synthesis and interpretation  
- [x] Reporting and communications  

## Critical evaluation
### Strengths
- Offers a clear **theoretical–practical bridge**: moves from Latour and Bowker & Star to concrete library practices.  
- Originality lies in the **rubric of addressability** (non-addressable, received, captured).  
- Anticipates the relevance of **Explainable AI** to LIS and digital humanities contexts.  

### Weaknesses and limitations
- Limited empirical testing: largely conceptual despite DLF grounding.  
- Focus on UK legal deposit may restrict transferability.  
- Leaves the implementation of critical addressability principles largely as **future work**.  

### Author’s credibility
- Paul Gooding (University of Glasgow), established in digital library user studies, LIS and DH. Co-authorship with Terras on NPLD evaluation adds weight.  

### Contextual validity
- Conceptually generalisable; practical implications depend on infrastructures (e.g., EU vs. US privacy regimes).  
- Stronger in framing questions than in delivering solutions.  

### Comparisons
- Echoes **Bowker & Star’s** classification-as-infrastructure critique.  
- Aligns with **Drucker’s** notion of capta, not data.  
- Extends **Jaillant’s** work on incomplete data into a methodological proposal.  
- Resonates with **Archer’s DDR struggles** over prescriptive versus emergent methods: both expose the limits of black-boxed models.  

## Interpretation
### Analysis and insights
- The metaphor of black boxes in LIS resonates directly with DDR’s **systematic models**: Archer’s diagrams also risked obscuring tacit labour, assumptions and instabilities.  
- Gooding’s insistence on **critical transparency** parallels the project’s re-evaluation of DDR: both argue for opening archival structures to critique before reuse.  
- Implication: project data (archival, contemporary, computational) should always be tested against the **addressability rubric** before being operationalised in simulations or decision-support tools.  
- Suggests a **layered reflexive practice**: when complete transparency is impossible (non-addressable data), researchers must explicitly document the limits.  

## Evidence to quote or paraphrase
- 'Many datasets are received as black boxes, with only the input and output known' (p. 126).  
- “Critical addressability refers to the notion that one should be able to evaluate the technical and social forces that shape data' (p. 119).  
- **Paraphrase:** Gooding argues that collections and catalogues should be reframed as *capta* (constructed, partial, situated), not raw, transparent data (p. 121)【20†source】.  

## Related works
- Bowker & Star (1999) *Sorting Things Out*.  
- Drucker (2011) on capta.  
- Risam (2015) on intersectionality in DH.  
- Stodden (2010) on reproducibility and black boxes.  
- FAIR Data Principles (Wilkinson et al., 2016).  
- Jaillant (2022) on digital archives and gaps.  
- Latour (1987) *Science in Action*.  

## Questions for further research
- How to develop **workflows and prototypes** for implementing critical addressability in design practice contexts?  
- Can the **three layers of addressability** be mapped onto DDR archival practices (fixed briefs, unstable data, reflexive artefact-making)?  
- How can ML/RAG systems be designed to expose rather than obscure the provenance and transformations of data?  
- To what extent can interdisciplinary collaborations (LIS + DH + AI) produce a new model of **explainable archival infrastructures**?  