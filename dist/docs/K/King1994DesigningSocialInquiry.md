---
title: "Designing social inquiry: scientific inference in qualitative research"
authors: "King, Gary; Keohane, Robert O.; Verba, Sidney"
year: 1994
journal: "Princeton University Press"
citation_key: King1994DesigningSocialInquiry
doi: ""
url: ""
bibliography: ../../refs/library.bib
csl: "https://www.zotero.org/styles/harvard-cite-them-right"
link-citations: true
last_updated: "Sep 04, 2025, 10:21 AM"---
## Purpose and aim
### What research question or objective is being addressed?
King et al. set out to **unify qualitative and quantitative research under a single logic of scientific inference**. Their central claim is that good research, regardless of method, pursues **descriptive and causal inference** through transparent procedures, explicit assumptions and replicable reasoning. They pose the question: *how can qualitative research be designed so that inferences are valid, unbiased and informative?* [@King1994DesigningSocialInquiry].

## Methodology
### Describe the research design, methods and sample size.
A **programmatic methodological treatise**, not an empirical study. The book synthesises statistical inference, philosophy of science and comparative politics to articulate rules for research design. It offers examples from small-N case studies, surveys and historical analysis to show how qualitative work may adopt the same logic of inference as statistical studies. King et al. distinguish **descriptive inference** (learning about an unobserved fact from an observed one) and **causal inference** (learning about causal effects from observations).

## Key findings and arguments
### Summarise the main results and conclusions
1. **Unity of logic.** There is no fundamental difference between qualitative and quantitative research; both should follow the same rules of inference.  
2. **Four aims of scientific inference.** Research should make clear claims, connect those claims to evidence, consider uncertainty and acknowledge the limits of inference.  
3. **Design over method.** The credibility of conclusions depends more on **research design** than on the data type. King et al. emphasise theory-driven designs, clear operationalisation and attention to the data-generating process.  
4. **Avoiding bias.** Common problems include **selection on the dependent variable**, endogenous selection, measurement error, post-treatment bias and over-controlled models.  
5. **Maximising leverage.** When N is small, increase the **number and variance of observations** → disaggregate cases across time or space → select cases to maximise variation in key explanatory variables → derive observable implications beyond the data at hand.  
6. **Transparency and replication.** Procedures should be public, data shared, coding rules explicit and uncertainty reported.  
7. **Causal thinking in small-N.** With few cases, test causal claims via process tracing, counterfactual reasoning and observable implications, provided the design guards against bias.  
8. **Model dependence and robustness.** Check inferences under alternative codings, measures and models; treat robustness analysis as a design obligation.

## Relevance
### How does it link to the research questions or framework?
- Provides a **foil** for the project’s **interpretivist–critical paradigm**. King et al.’s insistence on public procedures, explicit assumptions and bias diagnostics can **strengthen** the project’s methodological credibility without abandoning interpretive or critical aims.  
- Speaks directly to the **methods spine**: an emphasis on design before data aligns with the pipeline from framing → modelling → simulation → decision support → reflective synthesis.  
- Useful for the **critical taxonomy**: makes clear where classificatory choices (coding rules, variable definitions) shape inferences and reproduce power.  
- Aids the **statements of intent**: reconciles Archer’s systematic impulses with reflexive practice by specifying where systematisation helps (threats to validity) and where it does not (closing down meaning).

## Project integration
### Why it helps the project (evidence-linked)
- **Design first.** Treat research designs as first-class artefacts with explicit assumptions, data-generating processes and bias checks [@King1994DesigningSocialInquiry].  
- **Selection bias guardrails.** Build protocols to avoid selection on the dependent variable in archival and design cases (e.g., do not only sample ‘successful’ projects).  
- **Leverage with small-N.** When cases are few, **multiply observations** by temporal slices, sub-components or stakeholder perspectives; increase variance in key conditions to identify patterns.  
- **Measurement transparency.** Publish coding schemes for taxonomies, briefs and ‘known vs not known’ categories to make interpretive choices inspectable.  
- **Uncertainty reporting.** Pair qualitative claims with uncertainty statements and rival explanations; document where evidence is thin or contradictory.  
- **Robustness.** Re-analyse critical findings under alternative classifications or model specifications to test sensitivity.  

### Hooks into the project
- **Workstreams →** Methods spine design → Decision-support prototypes → Archival sampling → Critical taxonomy → Reflexive briefing  
- **Deliverables →** Design dossier template (theory, claims, data, biases, alternatives) → Selection-bias checklist → Robustness log → Uncertainty statements embedded in reports and visualisations  
- **Stakeholders →** Design researchers → Methodologists → Innovation labs → Policy partners

### Use across the methods spine
- [x] Framing and theory  
- [x] Study design  
- [x] Data collection and instruments  
- [x] Analysis and models  
- [x] Synthesis and interpretation  
- [x] Reporting and communications

## Critical evaluation
### Strengths
- **Clarity and discipline.** Offers a rigorous vocabulary for threats to inference that travels well across methods.  
- **Actionable guidance.** Concrete design moves for small-N research: maximise variance, add observations, derive new implications, make procedures public.  
- **Bridging stance.** Reduces unproductive method wars by insisting on shared standards of inference.

### Weaknesses and limitations
- **Positivist tilt.** Prioritises **explanation over understanding**, risking the flattening of meaning, context and power.  
- **Underplays reflexivity.** Gives limited guidance on positionality, standpoint or the co-construction of meaning central to interpretivism.  
- **Thin on ethics and politics.** Treats classification and measurement as technical, not as **political artefacts** in the Bowker & Star sense.  
- **Normative overreach.** The ‘one logic’ claim can become prescriptive, discounting plural epistemologies needed in community and Indigenous settings.

### Author’s credibility
Gary King is a leading methodologist in political science. Robert Keohane and Sidney Verba are senior figures in international relations and political methodology. The book is canonical and widely cited across the social sciences.

### Contextual validity
Highly applicable to comparative, historical and archival design research **when used critically**. King et al.’s tools for bias diagnosis and design transparency improve credibility, but require augmentation with interpretivist reflexivity and critical theory to avoid technocratic closure.

### Comparisons
- **Archer (1968).** Sought inference rules for design processes (evaluating options against objectives). King et al. ‘design before data’ sharpens Archer’s systematic impulse; Archer’s operational cycling supplies the reflexive loop missing from King et al.  
- **Freire (2014).** Praxis exposes how ‘neutral’ designs can reproduce domination; King et al. need an ethics of emancipation to complement their logic.  
- **Bowker & Star (1999).** Measurement and classification are moral-political infrastructures; coding rules are not neutral and must be contested.  
- **Williamson (2018).** Supplies the interpretivist-critical framework missing in King et al., connecting inference to meaning, power and justice.

## Interpretation
### Analysis and insights
- Use King et al. as a **discipline of transparency**, not a straightjacket. Their rules can upgrade credibility while the project remains interpretivist–critical.  
- Translate ‘increase observations’ into practice: break complex design projects into **episodes, artefacts and decisions** to expand N without losing depth.  
- Make **selection-bias auditing** routine in archival work: track which voices, failures and residuals are absent.  
- Treat coding and taxonomy as **sites of power**: publish and debate them with stakeholders; design for re-classification.  
- Produce **rival explanations** alongside primary narratives to embody uncertainty and pluralism.

## Evidence to quote or paraphrase
- ‘The differences between quantitative and qualitative research are only stylistic’ (general thesis, ch. 1).  
- ‘Our goal is inference… descriptive and causal’ (ch. 1).  
- Warnings against **selection on the dependent variable** and for increasing the **number and variance of observations** (chs. 3–4).  
- Emphasis on **public procedures**, data sharing and **replication** (ch. 1).  

## Related works
- Brady, Collier & Seawright (2004) *Rethinking Social Inquiry* (critical responses to King et al.).  
- Goertz & Mahoney (2012) *A Tale of Two Cultures*.  
- George & Bennett (2005) *Case Studies and Theory Development in the Social Sciences*.  
- King (1995, 2007) on transparency and replication.  
- Archer (1968); Freire (2014); Bowker & Star (1999); Williamson (2018).

## Questions for further research
- How to integrate King et al.’s bias-diagnostics with **interpretivist reflexivity** and **critical ethics** in one protocol?  
- What does a **selection-bias audit** look like for design archives and taxonomies?  
- How can the **Arena Simulator** incorporate King et al.’s advice on maximising variance and observable implications without reducing meaning?  
- Where do **community and Indigenous methodologies** resist or revise King et al.’s ‘one logic’ claim?  